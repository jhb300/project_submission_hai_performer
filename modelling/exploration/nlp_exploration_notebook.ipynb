{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exploration Notebook\n",
    "- Latent dirichlet allocation\n",
    "- Text Classification into fixed categories\n",
    "- Embed text and build clusers form the embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, ForeignKey, select\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class Experiment(Base):\n",
    "    __tablename__ = 'lda_experiment'\n",
    "    id = Column('id', Integer, primary_key=True)\n",
    "    model = Column('model_path', String(100))\n",
    "    dataset = Column('dataset_path', String(100))\n",
    "    num_topics = Column('num_topics', Integer)\n",
    "    epochs = Column('passes', Integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = create_engine('sqlite:///:memory:', echo=True)\n",
    "engine = create_engine(\"sqlite:///config/experiment_config.db\")\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_model_topics_5_passes_10\n",
      "lda_model_topics_10_passes_10\n",
      "lda_model_topics_50_passes_10\n",
      "lda_model_topics_100_passes_10\n",
      "lda_model_topics_500_passes_10\n",
      "lda_model_topics_1000_passes_10\n",
      "lda_model_topics_10_passes_20\n",
      "lda_model_topics_50_passes_20\n",
      "lda_model_topics_100_passes_20\n",
      "lda_model_topics_500_passes_20\n",
      "lda_model_topics_1000_passes_20\n",
      "lda_model_topics_10_passes_50\n",
      "lda_model_topics_50_passes_50\n",
      "lda_model_topics_100_passes_50\n",
      "lda_model_topics_500_passes_50\n",
      "lda_model_topics_1000_passes_50\n"
     ]
    }
   ],
   "source": [
    "q1 = select(Experiment)\n",
    "q1_result = session.execute(q1) \n",
    "for s in q1_result.scalars():\n",
    "    print(f\"{s.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Latent Dirichlet Allocation](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['daily',\n",
       "  'notebook',\n",
       "  'mike',\n",
       "  'santoli',\n",
       "  'cnbcs',\n",
       "  'senior',\n",
       "  'market',\n",
       "  'commentator',\n",
       "  'idea',\n",
       "  'trend',\n",
       "  'stock',\n",
       "  'market',\n",
       "  'statistic']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load processed data from csv\n",
    "dataset_name = 'cnbc_news_dataset_processed'\n",
    "dataset_path = f'../data_engineering/nlp_data/{dataset_name}.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "# Convert into list of lists\n",
    "processed_docs = []\n",
    "for i in list(df.short_description_lemmatized):\n",
    "    if type(i)==str:\n",
    "        processed_docs.append(eval(i))\n",
    "processed_docs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'cnbcs'), (1, 'commentator'), (2, 'daily'), (3, 'idea'), (4, 'market')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary from 'processed_docs' containing the number of times a word appears in the training\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "list(dictionary.iteritems())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words model for each document (dictionary per doc reporting how many words and how many times those words appear)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 1000\n",
    "passes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = num_topics, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = passes,\n",
    "                                   workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and configuration\n",
    "model_name = f'lda_model_topics_{num_topics}_passes_{passes}'\n",
    "model_path = f'./models/LDA/{model_name}'\n",
    "lda_model.save(model_path)\n",
    "experiment_config = Experiment(\n",
    "    model = model_name, \n",
    "    dataset = dataset_name, \n",
    "    num_topics=num_topics, \n",
    "    epochs=passes\n",
    ")\n",
    "session.add(experiment_config)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_on_doc(doc):\n",
    "    # Data preprocessing step for the unseen document\n",
    "    if type(doc)==str:\n",
    "        bow_vector = dictionary.doc2bow(eval(doc))\n",
    "        return lda_model[bow_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>short_description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>description</th>\n",
       "      <th>title_lowered</th>\n",
       "      <th>title_tokenized</th>\n",
       "      <th>title_removed_stopwords</th>\n",
       "      <th>title_lemmatized</th>\n",
       "      <th>short_description_lowered</th>\n",
       "      <th>short_description_tokenized</th>\n",
       "      <th>short_description_removed_stopwords</th>\n",
       "      <th>short_description_lemmatized</th>\n",
       "      <th>description_lowered</th>\n",
       "      <th>description_tokenized</th>\n",
       "      <th>description_removed_stopwords</th>\n",
       "      <th>description_lemmatized</th>\n",
       "      <th>topic_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Santoli’s Wednesday market notes: Could Septem...</td>\n",
       "      <td>2021-09-29T17:09:39+0000</td>\n",
       "      <td>This is the daily notebook of Mike Santoli, CN...</td>\n",
       "      <td>cnbc, Premium, Articles, Investment strategy, ...</td>\n",
       "      <td>This is the daily notebook of Mike Santoli, CN...</td>\n",
       "      <td>santoli’s wednesday market notes could septemb...</td>\n",
       "      <td>['santoli', '’', 's', 'wednesday', 'market', '...</td>\n",
       "      <td>['santoli', 'wednesday', 'market', 'notes', 'c...</td>\n",
       "      <td>['santoli', 'wednesday', 'market', 'note', 'co...</td>\n",
       "      <td>this is the daily notebook of mike santoli cnb...</td>\n",
       "      <td>['this', 'is', 'the', 'daily', 'notebook', 'of...</td>\n",
       "      <td>['daily', 'notebook', 'mike', 'santoli', 'cnbc...</td>\n",
       "      <td>['daily', 'notebook', 'mike', 'santoli', 'cnbc...</td>\n",
       "      <td>this is the daily notebook of mike santoli cnb...</td>\n",
       "      <td>['this', 'is', 'the', 'daily', 'notebook', 'of...</td>\n",
       "      <td>['daily', 'notebook', 'mike', 'santoli', 'cnbc...</td>\n",
       "      <td>['daily', 'notebook', 'mike', 'santoli', 'cnbc...</td>\n",
       "      <td>[(128, 0.091083094), (266, 0.09107704), (346, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>My take on the early Brexit winners and losers</td>\n",
       "      <td>2016-06-24T13:50:48-0400</td>\n",
       "      <td>This commentary originally ran on Facebook. Bo...</td>\n",
       "      <td>Articles, Politics, Europe News, European Cent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my take on the early brexit winners and losers</td>\n",
       "      <td>['my', 'take', 'on', 'the', 'early', 'brexit',...</td>\n",
       "      <td>['take', 'early', 'brexit', 'winners', 'losers']</td>\n",
       "      <td>['take', 'early', 'brexit', 'winner', 'loser']</td>\n",
       "      <td>this commentary originally ran on facebook bor...</td>\n",
       "      <td>['this', 'commentary', 'originally', 'ran', 'o...</td>\n",
       "      <td>['commentary', 'originally', 'ran', 'facebook'...</td>\n",
       "      <td>['commentary', 'originally', 'ran', 'facebook'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(402, 0.06519049), (444, 0.91871387)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Europe's recovery depends on Renzi's Italy</td>\n",
       "      <td>2014-03-25T13:29:45-0400</td>\n",
       "      <td>In spring, ambitious reforms began in Italy. U...</td>\n",
       "      <td>Articles, Business News, Economy, Europe Econo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>europes recovery depends on renzis italy</td>\n",
       "      <td>['europes', 'recovery', 'depends', 'on', 'renz...</td>\n",
       "      <td>['europes', 'recovery', 'depends', 'renzis', '...</td>\n",
       "      <td>['europe', 'recovery', 'depends', 'renzis', 'i...</td>\n",
       "      <td>in spring ambitious reforms began in italy und...</td>\n",
       "      <td>['in', 'spring', 'ambitious', 'reforms', 'bega...</td>\n",
       "      <td>['spring', 'ambitious', 'reforms', 'began', 'i...</td>\n",
       "      <td>['spring', 'ambitious', 'reform', 'began', 'it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(14, 0.07672071), (78, 0.033589777), (122, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  Santoli’s Wednesday market notes: Could Septem...   \n",
       "1           1     My take on the early Brexit winners and losers   \n",
       "2           2         Europe's recovery depends on Renzi's Italy   \n",
       "\n",
       "               published_at  \\\n",
       "0  2021-09-29T17:09:39+0000   \n",
       "1  2016-06-24T13:50:48-0400   \n",
       "2  2014-03-25T13:29:45-0400   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  This is the daily notebook of Mike Santoli, CN...   \n",
       "1  This commentary originally ran on Facebook. Bo...   \n",
       "2  In spring, ambitious reforms began in Italy. U...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  cnbc, Premium, Articles, Investment strategy, ...   \n",
       "1  Articles, Politics, Europe News, European Cent...   \n",
       "2  Articles, Business News, Economy, Europe Econo...   \n",
       "\n",
       "                                         description  \\\n",
       "0  This is the daily notebook of Mike Santoli, CN...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "\n",
       "                                       title_lowered  \\\n",
       "0  santoli’s wednesday market notes could septemb...   \n",
       "1     my take on the early brexit winners and losers   \n",
       "2           europes recovery depends on renzis italy   \n",
       "\n",
       "                                     title_tokenized  \\\n",
       "0  ['santoli', '’', 's', 'wednesday', 'market', '...   \n",
       "1  ['my', 'take', 'on', 'the', 'early', 'brexit',...   \n",
       "2  ['europes', 'recovery', 'depends', 'on', 'renz...   \n",
       "\n",
       "                             title_removed_stopwords  \\\n",
       "0  ['santoli', 'wednesday', 'market', 'notes', 'c...   \n",
       "1   ['take', 'early', 'brexit', 'winners', 'losers']   \n",
       "2  ['europes', 'recovery', 'depends', 'renzis', '...   \n",
       "\n",
       "                                    title_lemmatized  \\\n",
       "0  ['santoli', 'wednesday', 'market', 'note', 'co...   \n",
       "1     ['take', 'early', 'brexit', 'winner', 'loser']   \n",
       "2  ['europe', 'recovery', 'depends', 'renzis', 'i...   \n",
       "\n",
       "                           short_description_lowered  \\\n",
       "0  this is the daily notebook of mike santoli cnb...   \n",
       "1  this commentary originally ran on facebook bor...   \n",
       "2  in spring ambitious reforms began in italy und...   \n",
       "\n",
       "                         short_description_tokenized  \\\n",
       "0  ['this', 'is', 'the', 'daily', 'notebook', 'of...   \n",
       "1  ['this', 'commentary', 'originally', 'ran', 'o...   \n",
       "2  ['in', 'spring', 'ambitious', 'reforms', 'bega...   \n",
       "\n",
       "                 short_description_removed_stopwords  \\\n",
       "0  ['daily', 'notebook', 'mike', 'santoli', 'cnbc...   \n",
       "1  ['commentary', 'originally', 'ran', 'facebook'...   \n",
       "2  ['spring', 'ambitious', 'reforms', 'began', 'i...   \n",
       "\n",
       "                        short_description_lemmatized  \\\n",
       "0  ['daily', 'notebook', 'mike', 'santoli', 'cnbc...   \n",
       "1  ['commentary', 'originally', 'ran', 'facebook'...   \n",
       "2  ['spring', 'ambitious', 'reform', 'began', 'it...   \n",
       "\n",
       "                                 description_lowered  \\\n",
       "0  this is the daily notebook of mike santoli cnb...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "\n",
       "                               description_tokenized  \\\n",
       "0  ['this', 'is', 'the', 'daily', 'notebook', 'of...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "\n",
       "                       description_removed_stopwords  \\\n",
       "0  ['daily', 'notebook', 'mike', 'santoli', 'cnbc...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "\n",
       "                              description_lemmatized  \\\n",
       "0  ['daily', 'notebook', 'mike', 'santoli', 'cnbc...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "\n",
       "                                         topic_class  \n",
       "0  [(128, 0.091083094), (266, 0.09107704), (346, ...  \n",
       "1             [(402, 0.06519049), (444, 0.91871387)]  \n",
       "2  [(14, 0.07672071), (78, 0.033589777), (122, 0....  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic_class'] = df['short_description_lemmatized'].apply(lambda x: make_inference_on_doc(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_at</th>\n",
       "      <th>topic_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-29T17:09:39+0000</td>\n",
       "      <td>[(128, 0.091083094), (266, 0.09107704), (346, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-06-24T13:50:48-0400</td>\n",
       "      <td>[(402, 0.06519049), (444, 0.91871387)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-03-25T13:29:45-0400</td>\n",
       "      <td>[(14, 0.07672071), (78, 0.033589777), (122, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-04-22T19:49:03+0000</td>\n",
       "      <td>[(5, 0.19374597), (227, 0.074674495), (253, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-14T14:59:04+0000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               published_at                                        topic_class\n",
       "0  2021-09-29T17:09:39+0000  [(128, 0.091083094), (266, 0.09107704), (346, ...\n",
       "1  2016-06-24T13:50:48-0400             [(402, 0.06519049), (444, 0.91871387)]\n",
       "2  2014-03-25T13:29:45-0400  [(14, 0.07672071), (78, 0.033589777), (122, 0....\n",
       "3  2009-04-22T19:49:03+0000  [(5, 0.19374597), (227, 0.074674495), (253, 0....\n",
       "4  2018-04-14T14:59:04+0000                                               None"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = df[['published_at','topic_class']]\n",
    "df_out.to_csv(f\"./output_data/data_{model_name}_{dataset_name}.csv\")\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Top2Vec](https://top2vec.readthedocs.io/en/stable/Top2Vec.html#how-does-it-work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from top2vec import Top2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from csv\n",
    "dataset_name = 'cnbc_news_dataset_processed'\n",
    "dataset_path = f'../../data_engineering/nlp_data/{dataset_name}.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "documents = list(df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Define Model with pretrained encoder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m Top2Vec(documents, embedding_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniversal-sentence-encoder\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/top2vec/Top2Vec.py:574\u001b[0m, in \u001b[0;36mTop2Vec.__init__\u001b[0;34m(self, documents, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m embedding_model\n\u001b[0;32m--> 574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_import_status()\n\u001b[1;32m    576\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mPre-processing documents for training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    578\u001b[0m \u001b[39m# preprocess documents\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/top2vec/Top2Vec.py:1046\u001b[0m, in \u001b[0;36mTop2Vec._check_import_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39min\u001b[39;00m use_models:\n\u001b[1;32m   1045\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _HAVE_TENSORFLOW:\n\u001b[0;32m-> 1046\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model\u001b[39m}\u001b[39;00m\u001b[39m is not available.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39mTry: pip install top2vec[sentence_encoders]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39mAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39min\u001b[39;00m sbert_models:\n\u001b[1;32m   1050\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _HAVE_TORCH:\n",
      "\u001b[0;31mImportError\u001b[0m: universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text"
     ]
    }
   ],
   "source": [
    "# Define Model with pretrained encoder\n",
    "model = Top2Vec(documents, embedding_model='universal-sentence-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
