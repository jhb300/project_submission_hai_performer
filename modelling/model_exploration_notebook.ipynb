{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Exploration Notebook\n",
    "- Latent dirichlet allocation\n",
    "- Text Classification into fixed categories\n",
    "- Embed text and build clusers form the embedding space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, ForeignKey, select\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class Experiment(Base):\n",
    "    __tablename__ = 'lda_experiment'\n",
    "    id = Column('id', Integer, primary_key=True)\n",
    "    model = Column('model_path', String(100))\n",
    "    dataset = Column('dataset_path', String(100))\n",
    "    num_topics = Column('num_topics', Integer)\n",
    "    passes = Column('passes', Integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = create_engine('sqlite:///:memory:', echo=True)\n",
    "engine = create_engine(\"sqlite:///experiment_config.db\") # To store database persistently\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = select(Experiment).where(Experiment.dataset=='solar')\n",
    "q1_result = session.execute(q1) \n",
    "for s in q1_result.scalars():\n",
    "    print(f\"{s.dataset} {s.configuration_2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['daily',\n",
       "  'notebook',\n",
       "  'mike',\n",
       "  'santoli',\n",
       "  'cnbcs',\n",
       "  'senior',\n",
       "  'market',\n",
       "  'commentator',\n",
       "  'idea',\n",
       "  'trend',\n",
       "  'stock',\n",
       "  'market',\n",
       "  'statistic']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load processed data from csv\n",
    "dataset_name = '../data_engineering/nlp_data/cnbc_news_dataset_processed.csv'\n",
    "df = pd.read_csv(dataset_name)\n",
    "# Convert into list of lists\n",
    "processed_docs = []\n",
    "for i in list(df.short_description_lemmatized):\n",
    "    if type(i)==str:\n",
    "        processed_docs.append(eval(i))\n",
    "processed_docs[:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Latent Dirichlet Allocation](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'cnbcs'), (1, 'commentator'), (2, 'daily'), (3, 'idea'), (4, 'market')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary from 'processed_docs' containing the number of times a word appears in the training\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "list(dictionary.iteritems())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words model for each document (dictionary per doc reporting how many words and how many times those words appear)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "passes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = num_topics, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = passes,\n",
    "                                   workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/LDA/lda_model_topics_5_passes_10.state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/gensim/utils.py:764\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     _pickle\u001b[39m.\u001b[39mdump(\u001b[39mself\u001b[39m, fname_or_handle, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    765\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39msaved \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Save model and configuration\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./models/LDA/lda_model_topics_\u001b[39m\u001b[39m{\u001b[39;00mnum_topics\u001b[39m}\u001b[39;00m\u001b[39m_passes_\u001b[39m\u001b[39m{\u001b[39;00mpasses\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m lda_model\u001b[39m.\u001b[39msave(model_path)\n\u001b[1;32m      4\u001b[0m experiment_config \u001b[39m=\u001b[39m Experiment(model \u001b[39m=\u001b[39m model_path, dataset \u001b[39m=\u001b[39m dataset_name, num_topics\u001b[39m=\u001b[39mnum_topics, passes\u001b[39m=\u001b[39mpasses)\n\u001b[1;32m      5\u001b[0m session\u001b[39m.\u001b[39madd(experiment_config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/gensim/models/ldamodel.py:1596\u001b[0m, in \u001b[0;36mLdaModel.save\u001b[0;34m(self, fname, ignore, separately, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save the model to a file.\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \n\u001b[1;32m   1554\u001b[0m \u001b[39mLarge internal arrays may be stored into separate files, with `fname` as prefix.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \n\u001b[1;32m   1594\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1596\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39msave(utils\u001b[39m.\u001b[39msmart_extension(fname, \u001b[39m'\u001b[39m\u001b[39m.state\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1597\u001b[0m \u001b[39m# Save the dictionary separately if not in 'ignore'.\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mid2word\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ignore:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/gensim/utils.py:767\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    765\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39msaved \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    766\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `fname_or_handle` does not have write attribute\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_protocol\u001b[39m=\u001b[39mpickle_protocol)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/gensim/utils.py:611\u001b[0m, in \u001b[0;36mSaveLoad._smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    607\u001b[0m restores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_specials(\n\u001b[1;32m    608\u001b[0m     fname, separately, sep_limit, ignore, pickle_protocol, compress, subname,\n\u001b[1;32m    609\u001b[0m )\n\u001b[1;32m    610\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     pickle(\u001b[39mself\u001b[39m, fname, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    612\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[39m# restore attribs handled specially\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[39mfor\u001b[39;00m obj, asides \u001b[39min\u001b[39;00m restores:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/gensim/utils.py:1442\u001b[0m, in \u001b[0;36mpickle\u001b[0;34m(obj, fname, protocol)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpickle\u001b[39m(obj, fname, protocol\u001b[39m=\u001b[39mPICKLE_PROTOCOL):\n\u001b[1;32m   1430\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Pickle object `obj` to file `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \n\u001b[1;32m   1432\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \n\u001b[1;32m   1441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fname, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fout:  \u001b[39m# 'b' for binary, needed on Windows\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m         _pickle\u001b[39m.\u001b[39mdump(obj, fout, protocol\u001b[39m=\u001b[39mprotocol)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    189\u001b[0m     uri,\n\u001b[1;32m    190\u001b[0m     mode,\n\u001b[1;32m    191\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39mbuffering,\n\u001b[1;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    194\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    195\u001b[0m     newline\u001b[39m=\u001b[39mnewline,\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datascience/lib/python3.11/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39mbuffering, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/LDA/lda_model_topics_5_passes_10.state'"
     ]
    }
   ],
   "source": [
    "# Save model and configuration\n",
    "model_path = f'./models/LDA/lda_model_topics_{num_topics}_passes_{passes}'\n",
    "lda_model.save(model_path)\n",
    "experiment_config = Experiment(model = model_path, dataset = dataset_name, num_topics=num_topics, passes=passes)\n",
    "session.add(experiment_config)\n",
    "session.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.006*\"percent\" + 0.006*\"u\" + 0.004*\"billion\" + 0.004*\"bank\" + 0.003*\"said\" + 0.003*\"week\" + 0.003*\"market\" + 0.003*\"12\" + 0.003*\"change\" + 0.003*\"year\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.010*\"said\" + 0.010*\"percent\" + 0.006*\"year\" + 0.006*\"rate\" + 0.005*\"u\" + 0.005*\"market\" + 0.005*\"bank\" + 0.004*\"month\" + 0.004*\"new\" + 0.004*\"would\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.011*\"company\" + 0.010*\"said\" + 0.005*\"people\" + 0.004*\"u\" + 0.004*\"new\" + 0.004*\"year\" + 0.003*\"business\" + 0.003*\"million\" + 0.003*\"technology\" + 0.003*\"sale\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.009*\"market\" + 0.008*\"long\" + 0.008*\"said\" + 0.007*\"stock\" + 0.005*\"u\" + 0.005*\"year\" + 0.004*\"state\" + 0.004*\"money\" + 0.004*\"investor\" + 0.004*\"new\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"percent\" + 0.008*\"said\" + 0.008*\"stock\" + 0.007*\"market\" + 0.006*\"oil\" + 0.005*\"price\" + 0.005*\"year\" + 0.005*\"company\" + 0.004*\"u\" + 0.004*\"cramer\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.009*\"said\" + 0.005*\"trump\" + 0.005*\"one\" + 0.005*\"would\" + 0.004*\"cnbc\" + 0.004*\"year\" + 0.004*\"think\" + 0.004*\"business\" + 0.004*\"u\" + 0.004*\"new\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.013*\"said\" + 0.007*\"company\" + 0.006*\"year\" + 0.005*\"percent\" + 0.004*\"share\" + 0.004*\"investor\" + 0.004*\"u\" + 0.004*\"stock\" + 0.004*\"new\" + 0.003*\"market\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.012*\"share\" + 0.009*\"said\" + 0.009*\"percent\" + 0.007*\"cent\" + 0.007*\"revenue\" + 0.007*\"company\" + 0.006*\"per\" + 0.006*\"billion\" + 0.005*\"earnings\" + 0.005*\"year\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Data preprocessing step for the unseen document\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bow_vector \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39mdoc2bow(preprocess(unseen_document))\n\u001b[1;32m      3\u001b[0m \u001b[39m# Make inference on unseen document\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m index, score \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(lda_model[bow_vector], key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m tup: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m*\u001b[39mtup[\u001b[39m1\u001b[39m]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "# Make inference on unseen document\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
